{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.sys.path.insert(0, '/home/schirrmr/code/invertible-public/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invertible Networks for EEG Decoding Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminative and Generative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminative classification models learn $p(y|x)$, to predict the class given an input:\n",
    "\n",
    "Data | $p(y|x)$\n",
    "- | - \n",
    "![](./data.png) | ![](./relclass.png)\n",
    "\n",
    "\n",
    "Explicit generative class-conditional models learn $p(y,x)$ to predict the joint density of a given class and a given input:\n",
    "\n",
    "\n",
    "\n",
    "$p(y,0)$ | $p(y,1)$\n",
    "- | -\n",
    "![](./class0.png) | ![](./class1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see what advantages invertible networks as generative models offer.\n",
    "First, we show a simplified EEG example to illustrate how invertible networks work. Alpha power from 2 Electrodes to distinguish right hand movement from resting state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load High-Gamma Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the high-gamma dataset ([paper](http://onlinelibrary.wiley.com/doi/10.1002/hbm.23730/full), [data](https://gin.g-node.org/robintibor/high-gamma-dataset)) through the EEG deep learning library [Braindecode](braindecode.org/).\n",
    "We will use a simple 2d EEG example with right hand movements and resting state as the two signal classes. We will extract the alpha power from two the C3 and C4 sensors, which is very informative for resting state vs right hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braindecode\n",
    "from braindecode.datasets.moabb import MOABBDataset\n",
    "from braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess\n",
    "\n",
    "\n",
    "subject_id = 4\n",
    "\n",
    "# using the moabb dataset to load our data\n",
    "dataset = MOABBDataset(dataset_name=\"Schirrmeister2017\", subject_ids=[subject_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from braindecode.datautil.preprocess import exponential_moving_demean\n",
    "# making a copy just to be able to rerun preprocessing without\n",
    "# waiting later\n",
    "preproced_set = deepcopy(dataset)\n",
    "low_cut_hz = 7.  # low cut frequency for filtering\n",
    "high_cut_hz = 14.  # high cut frequency for filtering\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessors = [\n",
    "    # convert from volt to microvolt, directly modifying the numpy array\n",
    "    NumpyPreproc(fn=lambda x: x * 1e6),\n",
    "    # keep only EEG sensors\n",
    "    NumpyPreproc(fn=exponential_moving_demean, init_block_size=1000, factor_new=1e-3),\n",
    "    MNEPreproc(fn='resample', sfreq=50),\n",
    "    MNEPreproc(fn='pick_channels', ch_names=['C3', 'C4',], ordered=True),\n",
    "    # bandpass filter\n",
    "    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    MNEPreproc(fn='resample', sfreq=32),\n",
    "]\n",
    "\n",
    "# Preprocess the data\n",
    "preprocess(preproced_set, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.windowers import create_windows_from_events\n",
    "# Next, extract the 4-second trials from the dataset.\n",
    "# Create windows using braindecode function for this. It needs parameters to define how\n",
    "# trials should be used.\n",
    "class_names = ['Right Hand', 'Rest'] # for later plotting\n",
    "class_mapping = {'right_hand': 0, 'rest': 1}\n",
    "\n",
    "windows_dataset = create_windows_from_events(\n",
    "    preproced_set,\n",
    "    trial_start_offset_samples=0,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    "    mapping=class_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look inside the dataset to see what it can be splitted by and split into training, validation and evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_dataset.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "splitted = windows_dataset.split('run')\n",
    "train_set = splitted['train']\n",
    "n_split = int(np.round(0.8 * len(train_set)))\n",
    "valid_set = Subset(train_set, range(n_split,len(train_set)))\n",
    "train_set = Subset(train_set, range(0, n_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract mean squared values to have an estimate of the power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.utils import to_numpy\n",
    "from skorch.utils import to_tensor\n",
    "\n",
    "train_X = np.stack([to_numpy(X) for X,y,i in train_set], axis=0)\n",
    "train_y = np.stack([y for X,y,i in train_set], axis=0)\n",
    "\n",
    "mean_squared_X = np.mean(np.square(train_X), axis=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn\n",
    "seaborn.set_palette('colorblind')\n",
    "seaborn.set_style('darkgrid')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "#matplotlib.rcParams['figure.figsize'] = (12.0, 1.0)\n",
    "matplotlib.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the data is well-discriminable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "for i_class in range(2):\n",
    "    mask = train_y==i_class\n",
    "    plt.scatter(mean_squared_X[mask][:,0], mean_squared_X[mask][:,1],\n",
    "               label=class_names[i_class], s=22, alpha=.7)\n",
    "plt.legend()\n",
    "plt.xlabel(\"C3 Mean Squared Alpha Signal [mV]\")\n",
    "plt.ylabel(\"C4 Mean Squared Alpha Signal [mV]\")\n",
    "plt.title(\"Real Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Invertible Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a small invertible network to train on this data and learn the underlying distribution. Invertible networks work by transforming the input data invertibly/bijectively to a predefined distribition, e.g., a gaussian distribution.\n",
    "\n",
    "One prominent example for an invertible transformation is a coupling block:\n",
    "\n",
    "1. Split input $x$ by dimensions into disjoint parts $x_1$, $x_2$\n",
    "2. Compute $x_1' = f(x_2) * x_1 + g(x_2)$ with $f, g$ arbitrary functions (e.g. neural networks)\n",
    "3. Concatenate to $x' = (x_1', x_2)$\n",
    "\n",
    "Inversion  is possible via $x_1 = \\frac{x_1' - g(x_2)}{f(x_2)}$\n",
    "\n",
    "Find further information in these blog articles:\n",
    "* https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html\n",
    "* https://blog.evjang.com/2018/01/nf1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from invertible.affine import AffineCoefs, AffineModifier\n",
    "from invertible.coupling import  CouplingLayer\n",
    "from invertible.split_merge import ChunkChansIn2, EverySecondChan\n",
    "from invertible.sequential import InvertibleSequential\n",
    "from invertible.permute import InvPermute\n",
    "from invertible.actnorm import ActNorm\n",
    "from torch import nn\n",
    "from skorch.utils import to_tensor\n",
    "import torch as th\n",
    "from invertible.init import init_all_modules\n",
    "from invertible.distribution import NClassIndependentDist\n",
    "from braindecode.util import set_random_seeds\n",
    "\n",
    "\n",
    "def get_X_y_th(dataset):\n",
    "    X_np = np.stack([to_numpy(X) for X,y,i in dataset], axis=0)\n",
    "    y_np = np.stack([y for X,y,i in dataset], axis=0)\n",
    "    mean_squared_X = np.mean(np.square(X_np), axis=2) / 100\n",
    "    X_th = to_tensor(mean_squared_X, 'cpu')\n",
    "    y_th = th.nn.functional.one_hot(\n",
    "        to_tensor(y_np, 'cpu'), num_classes=2)\n",
    "    return X_th, y_th\n",
    "\n",
    "def flow_block():\n",
    "    return InvertibleSequential(ActNorm(2, 'exp', ),\n",
    "        InvPermute(2,fixed=False,use_lu=True),\n",
    "        CouplingLayer(\n",
    "            ChunkChansIn2(swap_dims=False), \n",
    "            AffineCoefs(nn.Sequential(\n",
    "                nn.Linear(1,512),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(512,2),\n",
    "             ), EverySecondChan()),\n",
    "            AffineModifier('sigmoid', add_first=True, eps=0)))\n",
    "\n",
    "set_random_seeds(20200617, False)\n",
    "\n",
    "net = InvertibleSequential(\n",
    "    flow_block(),\n",
    "    flow_block(),\n",
    "    flow_block(),\n",
    "    flow_block(),\n",
    "    NClassIndependentDist(2, 2),\n",
    ").cuda()\n",
    "\n",
    "\n",
    "init_all_modules(net, None)\n",
    "optim = th.optim.Adam(net.parameters(), lr=1e-3, weight_decay=5e-5)\n",
    "\n",
    "\n",
    "train_X_th, train_y_th = get_X_y_th(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Invertible Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the network to maximize the likelihood of the data under our prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "for i_epoch in range(n_epochs+1):\n",
    "    noise = th.randn_like(train_X_th) * 1e-3\n",
    "    noised = train_X_th + noise\n",
    "    z, lp = net(noised.cuda(), fixed=dict(y=train_y_th.cuda()))\n",
    "    nll = -th.mean(lp)\n",
    "    nll.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    if i_epoch % (n_epochs // 10) == 0:\n",
    "        print(f\"Negative Log Likelihood: {nll.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can generate samples from the learned distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(2, False)\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "for i_class in range(2):\n",
    "    inv_x, _ = net.invert(None, fixed=dict(n_samples=200, y=i_class))\n",
    "    inv_x = to_numpy(inv_x)\n",
    "    plt.scatter(inv_x[:,0], inv_x[:,1], s=22, alpha=.7, label=class_names[i_class])\n",
    "plt.legend()\n",
    "plt.xlabel(\"C3 Mean Squared Alpha Signal [µV]\")\n",
    "plt.ylabel(\"C4 Mean Squared Alpha Signal [µV]\")\n",
    "plt.title(\"Generated Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the class-conditional distributions $p(x|y)$ and the predictions $p(y|x)$ for any point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_start = 0\n",
    "x_stop = 1\n",
    "y_start = 0\n",
    "y_stop = 1.25\n",
    "points = th.stack(th.meshgrid(th.linspace(x_start,x_stop,30),\n",
    "                              th.linspace(y_start,y_stop,30)), dim=-1)\n",
    "lprobs = net(points.view(-1, points.shape[-1]).cuda())[1]\n",
    "\n",
    "probs_grid_0 = th.exp(lprobs[:,0].view(points.shape[:2]))\n",
    "probs_grid_1 = th.exp(lprobs[:,1].view(points.shape[:2]))\n",
    "rel_probs_grid = th.softmax(lprobs, dim=1)[:,1].view(points.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "def create_cmap(c1,c2, n_bins):\n",
    "    alphas = np.linspace(0,1,100)\n",
    "    colors = np.array(c1)[None] * (1-alphas[:,None]) + np.array(c2)[None] * (alphas[:,None])\n",
    "    cmap = LinearSegmentedColormap.from_list('', colors)\n",
    "    return cmap\n",
    "cmap_bl = create_cmap([1,1,1], seaborn.color_palette()[0], 100)\n",
    "cmap_or = create_cmap([1,1,1], seaborn.color_palette()[1], 100)\n",
    "cmap_blor = create_cmap(seaborn.color_palette()[0], seaborn.color_palette()[1], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize=(14,4), sharex=True, sharey=True)\n",
    "im = axes[0].imshow(to_numpy(probs_grid_0).T, origin='lower left',\n",
    "                   extent=((x_start, x_stop, y_start, y_stop)), cmap=cmap_bl,\n",
    "                  aspect='auto', interpolation='bilinear', vmin=0, vmax=10)\n",
    "im2 = axes[1].imshow(to_numpy(probs_grid_1).T, origin='lower left',\n",
    "                   extent=((x_start, x_stop, y_start, y_stop)), cmap=cmap_or,\n",
    "                  aspect='auto', interpolation='bilinear', vmin=0, vmax=10)\n",
    "im2 = axes[2].imshow(to_numpy(rel_probs_grid).T, origin='lower left',\n",
    "                   extent=((x_start, x_stop, y_start, y_stop)), cmap=cmap_blor,\n",
    "                  aspect='auto', interpolation='bilinear',vmin=0,vmax=1)\n",
    "axes[0].set_title(\"Right Hand Learned Distribution\")\n",
    "axes[1].set_title(\"Rest Learned Distribution\")\n",
    "axes[2].set_title(\"Conditional Class Prob\")\n",
    "axes[1].set_xlabel(\"C3 Mean Squared Alpha Signal [µV]\")\n",
    "axes[0].set_ylabel(\"C4 Mean Squared Alpha Signal [µV]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also show the transformation the invertible network has learned, you can see it transforms the input data to two gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import ConnectionPatch\n",
    "fig, axes = plt.subplots(1,2, figsize=(8,4))\n",
    "with th.no_grad():\n",
    "    z, _ = net(train_X_th.cuda())\n",
    "    for i_class in range(2):\n",
    "        mask = train_y == i_class\n",
    "        \n",
    "        axes[0].scatter(train_X_th[mask][:,0], train_X_th[mask][:,1],\n",
    "                   label=class_names[i_class], s=22, alpha=.7,\n",
    "                       color=seaborn.color_palette()[i_class])\n",
    "        axes[1].scatter(*to_numpy(z[mask]).T, color=seaborn.color_palette()[i_class],\n",
    "                       s=22, alpha=.7)\n",
    "        for xb, xa in zip(to_numpy(train_X_th[mask].squeeze()), to_numpy(z[mask])):\n",
    "            con = ConnectionPatch(xyA=xa, xyB=xb, coordsA=\"data\", coordsB=\"data\",\n",
    "                                  axesA=axes[1], axesB=axes[0], color=\"grey\", lw=0.2)\n",
    "            axes[1].add_artist(con)\n",
    "axes[0].set_title(\"Input Data\")\n",
    "axes[1].set_title(\"Output Data\")\n",
    "fig.suptitle(\"Learned Invertible Transformation\", y=1.08, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can also evaluate the accuracy of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X_th, valid_y_th = get_X_y_th(valid_set)\n",
    "\n",
    "for setname, set_X, set_y in (('train', train_X_th, train_y_th),\n",
    "                                ('valid', valid_X_th, valid_y_th)):\n",
    "\n",
    "    with th.no_grad():\n",
    "        z,lp = net(set_X.cuda())\n",
    "        acc = np.mean(to_numpy(lp.argmax(dim=1) == set_y.cuda().argmax(dim=1)))\n",
    "    print(f\"{setname.capitalize()} Acc: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go to raw data, still simplified: 32 Hz, 2 second windows, single-sensor data, bandpassed between 7 and 14 Hz (alpha band)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from invertible.affine import AdditiveCoefs\n",
    "from invertible.view_as import Flatten2d\n",
    "from invertible.datautil import PreprocessedLoader\n",
    "from invertible.pure_model import NoLogDet\n",
    "from invertible.noise import GaussianNoise\n",
    "from invertible.subsample_split import SubsampleSplitter\n",
    "\n",
    "def conv_flow_block(n_chans):\n",
    "    return InvertibleSequential(\n",
    "        ActNorm(n_chans, 'exp', ),\n",
    "        InvPermute(n_chans,fixed=False,use_lu=True),\n",
    "        CouplingLayer(\n",
    "            ChunkChansIn2(swap_dims=False), \n",
    "            AdditiveCoefs(nn.Sequential(\n",
    "                nn.Conv1d(n_chans//2,128,7, padding=3),\n",
    "                nn.ELU(),\n",
    "                nn.Conv1d(128,n_chans//2,7, padding=3),\n",
    "             )),\n",
    "            AffineModifier('sigmoid', add_first=True, eps=0)))\n",
    "\n",
    "def dense_flow_block(n_chans):\n",
    "    return InvertibleSequential(ActNorm(n_chans, 'exp', ),\n",
    "        InvPermute(n_chans,fixed=False,use_lu=True),\n",
    "        CouplingLayer(\n",
    "            ChunkChansIn2(swap_dims=False), \n",
    "            AdditiveCoefs(nn.Sequential(\n",
    "                nn.Linear(n_chans//2,512),\n",
    "                nn.ELU(),\n",
    "                nn.Linear(512,n_chans//2),\n",
    "             )),\n",
    "            AffineModifier('sigmoid', add_first=True, eps=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from braindecode.datautil.preprocess import exponential_moving_demean\n",
    "# making a copy just to be able to rerun preprocessing without\n",
    "# waiting later\n",
    "preproced_set = deepcopy(dataset)\n",
    "low_cut_hz = 7.  # low cut frequency for filtering\n",
    "high_cut_hz = 14.  # high cut frequency for filtering\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessors = [\n",
    "    # convert from volt to microvolt, directly modifying the numpy array\n",
    "    NumpyPreproc(fn=lambda x: x * 1e6),\n",
    "    NumpyPreproc(fn=exponential_moving_demean, init_block_size=1000, factor_new=1e-3),\n",
    "    MNEPreproc(fn='resample', sfreq=50),\n",
    "    MNEPreproc(fn='pick_channels', ch_names=['C3',], ordered=True),\n",
    "    # bandpass filter\n",
    "    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    MNEPreproc(fn='resample', sfreq=32),\n",
    "]\n",
    "\n",
    "# Preprocess the data\n",
    "preprocess(preproced_set, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_dataset = create_windows_from_events(\n",
    "    preproced_set,\n",
    "    trial_start_offset_samples=0,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    "    window_size_samples=None,\n",
    "    window_stride_samples=None,\n",
    "    mapping={'right_hand': 0, 'rest': 1},\n",
    ")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "splitted = windows_dataset.split('run')\n",
    "train_set = splitted['train']\n",
    "n_split = int(np.round(len(train_set) * 0.75))\n",
    "valid_set = Subset(train_set, range(n_split,len(train_set)))\n",
    "train_set = Subset(train_set, range(0,n_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Invertible Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"raw\" time-series data, we will use a convolutional invertible network inspired by the popular [Glow](https://openai.com/blog/glow/) invertible network architecture. We have a special block in the end to make it easier for the network to separate amplitude and phase into different latent dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from invertible.amp_phase import AmplitudePhase\n",
    "set_random_seeds(20200718, True)\n",
    "n_chans = 1\n",
    "net = InvertibleSequential(\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    conv_flow_block(n_chans*2),\n",
    "    conv_flow_block(n_chans*2),\n",
    "    conv_flow_block(n_chans*2),\n",
    "    conv_flow_block(n_chans*2),\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    conv_flow_block(n_chans*8),\n",
    "    conv_flow_block(n_chans*8),\n",
    "    conv_flow_block(n_chans*8),\n",
    "    conv_flow_block(n_chans*8),\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    Flatten2d(),\n",
    "    dense_flow_block(n_chans*64),\n",
    "    dense_flow_block(n_chans*64),\n",
    "    dense_flow_block(n_chans*64),\n",
    "    dense_flow_block(n_chans*64),\n",
    "    AmplitudePhase(),\n",
    "    NClassIndependentDist(2, n_chans*64, optimize_mean=True, optimize_std=False),\n",
    "\n",
    ").cuda()\n",
    "\n",
    "train_loader = th.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=50,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True)\n",
    "valid_loader = th.utils.data.DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=len(valid_set),\n",
    "    shuffle=False,\n",
    "    num_workers=0)\n",
    "preproced_loader = PreprocessedLoader(train_loader, GaussianNoise(1e-2), False)\n",
    "init_all_modules(net, th.cat([x[:,:,32:96] for x,y,i in preproced_loader], dim=0).cuda())\n",
    "\n",
    "optim = th.optim.Adam(net.parameters(), lr=5e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "rng = np.random.RandomState(394834)\n",
    "for i_epoch in range(n_epochs+1):\n",
    "    if i_epoch > 0:\n",
    "        for X_th, y, _ in train_loader:\n",
    "            start_ind = rng.randint(0,64)\n",
    "            X_th = X_th[:,:,start_ind:start_ind+64]\n",
    "            y_th = th.nn.functional.one_hot(y, num_classes=2).cuda()\n",
    "            noise = th.randn_like(X_th) * 1e-2#5e-1\n",
    "            noised = X_th + noise\n",
    "            z, lp = net(noised.cuda(), fixed=dict(y=None))\n",
    "            cross_ent = th.nn.functional.cross_entropy(\n",
    "                lp, y_th.argmax(dim=1),)\n",
    "            nll = -th.mean(th.sum(lp * y_th, dim=1))\n",
    "            loss = cross_ent * 10 + nll\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "    if (i_epoch % (n_epochs // 10) == 0) or (i_epoch == n_epochs):\n",
    "        print(i_epoch)\n",
    "        for X_th, y, _ in train_loader:\n",
    "            X_th = X_th[:,:,32:32+64]\n",
    "            y_th = th.nn.functional.one_hot(y, num_classes=2).cuda()\n",
    "            z, lp = net(X_th.cuda())\n",
    "        print(f\"Train NLL: {-th.mean(th.sum(lp *y_th, dim=1)).item():.1f}\")\n",
    "        print(f\"Train Acc: {to_numpy(y.cuda() == lp.argmax(dim=1)).mean():.1%}\")\n",
    "        for X_th, y, _ in valid_loader:\n",
    "            X_th = X_th[:,:,32:32+64]\n",
    "            y_th = th.nn.functional.one_hot(y, num_classes=2).cuda()\n",
    "            z, lp = net(X_th.cuda())\n",
    "        print(f\"Valid NLL: {-th.mean(th.sum(lp *y_th, dim=1)).item():.1f}\")\n",
    "        print(f\"Valid Acc: {to_numpy(y.cuda() == lp.argmax(dim=1)).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th.save(net.state_dict(), \"netstatetutorialeeg.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(th.load('netstatetutorialeeg.pth'))\n",
    "init_all_modules(net, None)\n",
    "_  = net(X_th.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate accuracy and Negative Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see improve daccuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, loader in (('Train', train_loader), ('Valid', valid_loader)):\n",
    "    all_lps = []\n",
    "    all_corrects = []\n",
    "    for X_th, y, _ in loader:\n",
    "        X_th = X_th[:,:,32:32+64]\n",
    "        y_th = th.nn.functional.one_hot(y, num_classes=2).cuda()\n",
    "        z, lp = net(X_th.cuda())\n",
    "        corrects = to_numpy(y.cuda() == lp.argmax(dim=1))\n",
    "        lps = to_numpy(th.sum(lp * y_th, dim=1))\n",
    "        all_lps.extend(lps)\n",
    "        all_corrects.extend(corrects)\n",
    "    acc = np.mean(all_corrects)\n",
    "    nll = -np.mean(all_lps)\n",
    "    print(f\"{name} NLL: {nll:.1f}\")\n",
    "    print(f\"{name} Acc: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize most likely inputs per class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the most likely inputs per class, and the input directly in the middle of the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = net.sequential[-1]\n",
    "overall_mean = th.mean(dist.class_means, dim=0)\n",
    "plt.figure(figsize=(12,2))\n",
    "\n",
    "plt.plot(to_numpy(dist.class_means.squeeze()).T)\n",
    "plt.plot(to_numpy(overall_mean.squeeze()))\n",
    "plt.legend(['Right', 'Rest', 'Neutral'], bbox_to_anchor=(1,1,0,0))\n",
    "plt.title(\"In Output Space\")\n",
    "\n",
    "plt.figure(figsize=(12,2))\n",
    "inved_mean, _ = net.invert(overall_mean.unsqueeze(0))\n",
    "inved_means, _ = net.invert(dist.class_means)\n",
    "\n",
    "plt.plot(to_numpy(inved_means.squeeze()).T)\n",
    "plt.plot(to_numpy(inved_mean.squeeze()))\n",
    "plt.legend(['Right', 'Rest', 'Neutral'], bbox_to_anchor=(1,1,0,0))\n",
    "plt.title(\"Inverted to Input Space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data without bandpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same for data without the bandpass and do more analysis on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a copy just to be able to rerun preprocessing without\n",
    "# waiting later\n",
    "preproced_set = deepcopy(dataset)\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessors = [\n",
    "    # convert from volt to microvolt, directly modifying the numpy array\n",
    "    NumpyPreproc(fn=lambda x: x * 1e6),\n",
    "    NumpyPreproc(fn=exponential_moving_demean, init_block_size=1000, factor_new=1e-3),\n",
    "    MNEPreproc(fn='pick_channels', ch_names=['C3',], ordered=True),\n",
    "    MNEPreproc(fn='resample', sfreq=32),\n",
    "]\n",
    "\n",
    "# Preprocess the data\n",
    "preprocess(preproced_set, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_dataset = create_windows_from_events(\n",
    "    preproced_set,\n",
    "    trial_start_offset_samples=0,\n",
    "    trial_stop_offset_samples=0,\n",
    "    preload=True,\n",
    "    window_size_samples=None,\n",
    "    window_stride_samples=None,\n",
    "    mapping=class_mapping,\n",
    ")\n",
    "splitted = windows_dataset.split('run')\n",
    "train_set = splitted['train']\n",
    "n_split = int(np.round(len(train_set) * 0.75))\n",
    "valid_set = Subset(train_set, range(n_split,len(train_set)))\n",
    "train_set = Subset(train_set, range(0,n_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from invertible.subsample_split import SubsampleSplitter\n",
    "set_random_seeds(20200718, True)\n",
    "n_chans = 1\n",
    "net = InvertibleSequential(\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    conv_flow_block(n_chans*2),\n",
    "    conv_flow_block(n_chans*2),\n",
    "    conv_flow_block(n_chans*2),\n",
    "    conv_flow_block(n_chans*2),\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    conv_flow_block(n_chans*8),\n",
    "    conv_flow_block(n_chans*8),\n",
    "    conv_flow_block(n_chans*8),\n",
    "    conv_flow_block(n_chans*8),\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    SubsampleSplitter((2,),chunk_chans_first=False),\n",
    "    Flatten2d(),\n",
    "    dense_flow_block(n_chans*64),\n",
    "    dense_flow_block(n_chans*64),\n",
    "    dense_flow_block(n_chans*64),\n",
    "    dense_flow_block(n_chans*64),\n",
    "    AmplitudePhase(),\n",
    "    NClassIndependentDist(2, n_chans*64, optimize_mean=True, optimize_std=False),\n",
    "\n",
    ").cuda()\n",
    "\n",
    "len(train_set)\n",
    "\n",
    "train_loader = th.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=50,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True)\n",
    "\n",
    "valid_loader = th.utils.data.DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=len(valid_set),\n",
    "    shuffle=False,\n",
    "    num_workers=0)\n",
    "preproced_loader = PreprocessedLoader(train_loader, GaussianNoise(1e-2), False)\n",
    "init_all_modules(net, th.cat([x[:,:,32:96] for x,y,i in preproced_loader], dim=0).cuda())\n",
    "\n",
    "optim = th.optim.Adam(net.parameters(), lr=5e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "rng = np.random.RandomState(394834)\n",
    "for i_epoch in range(n_epochs+1):\n",
    "    if i_epoch > 0:\n",
    "        for X_th, y, _ in train_loader:\n",
    "            start_ind = rng.randint(0,64)\n",
    "            X_th = X_th[:,:,start_ind:start_ind+64]\n",
    "            y_th = th.nn.functional.one_hot(y, num_classes=2).cuda()\n",
    "            noise = th.randn_like(X_th) * 1e-3#5e-1\n",
    "            noised = X_th + noise\n",
    "            z, lp = net(noised.cuda(), fixed=dict(y=None))\n",
    "            cross_ent = th.nn.functional.cross_entropy(\n",
    "                lp, y_th.argmax(dim=1),)\n",
    "            nll = -th.mean(th.sum(lp * y_th, dim=1))\n",
    "            loss = cross_ent * 10 + nll\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "    if (i_epoch % (n_epochs // 10) == 0) or (i_epoch == n_epochs):\n",
    "        print(i_epoch)\n",
    "        for X_th, y, _ in train_loader:\n",
    "            X_th = X_th[:,:,32:32+64]\n",
    "            y_th = th.nn.functional.one_hot(y, num_classes=2).cuda()\n",
    "            z, lp = net(X_th.cuda())\n",
    "        print(f\"Train NLL: {-th.mean(th.sum(lp *y_th, dim=1)).item():.1f}\")\n",
    "        print(f\"Train Acc: {to_numpy(y.cuda() == lp.argmax(dim=1)).mean():.1%}\")\n",
    "        for X_th, y, _ in valid_loader:\n",
    "            X_th = X_th[:,:,32:32+64]\n",
    "            y_th = th.nn.functional.one_hot(y, num_classes=2).cuda()\n",
    "            z, lp = net(X_th.cuda())\n",
    "        print(f\"Valid NLL: {-th.mean(th.sum(lp *y_th, dim=1)).item():.1f}\")\n",
    "        print(f\"Valid Acc: {to_numpy(y.cuda() == lp.argmax(dim=1)).mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th.save(net.state_dict(), \"netstatetutorialeegnolowpass.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(th.load('netstatetutorialeegnolowpass.pth'))\n",
    "init_all_modules(net, None)\n",
    "_ = net(th.zeros(1,1,64, device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, loader in (('Train', train_loader), ('Valid', valid_loader)):\n",
    "    all_lps = []\n",
    "    all_corrects = []\n",
    "    for X_th, y, _ in loader:\n",
    "        X_th = X_th[:,:,32:32+64]\n",
    "        y_th = th.nn.functional.one_hot(y, num_classes=2).cuda()\n",
    "        z, lp = net(X_th.cuda())\n",
    "        corrects = to_numpy(y.cuda() == lp.argmax(dim=1))\n",
    "        lps = to_numpy(th.sum(lp * y_th, dim=1))\n",
    "        all_lps.extend(lps)\n",
    "        all_corrects.extend(corrects)\n",
    "    acc = np.mean(all_corrects)\n",
    "    nll = -np.mean(all_lps)\n",
    "    print(f\"{name} NLL: {nll:.1f}\")\n",
    "    print(f\"{name} Acc: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that now the most likely inputs differ not only in alpha frequency but also in slower frequency/the trend over the window (higher start and lower end for the resting state class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = net.sequential[-1]\n",
    "overall_mean = th.mean(dist.class_means, dim=0)\n",
    "plt.figure(figsize=(12,2))\n",
    "\n",
    "plt.plot(to_numpy(dist.class_means.squeeze()).T)\n",
    "plt.plot(to_numpy(overall_mean.squeeze()))\n",
    "plt.legend(['Right', 'Rest', 'Neutral'], bbox_to_anchor=(1,1,0,0))\n",
    "plt.title(\"In Output Space\")\n",
    "\n",
    "plt.figure(figsize=(12,2))\n",
    "inved_mean, _ = net.invert(overall_mean.unsqueeze(0))\n",
    "inved_means, _ = net.invert(dist.class_means)\n",
    "\n",
    "plt.plot(to_numpy(inved_means.squeeze()).T)\n",
    "plt.plot(to_numpy(inved_mean.squeeze()))\n",
    "plt.legend(['Right', 'Rest', 'Neutral'], bbox_to_anchor=(1,1,0,0))\n",
    "plt.title(\"Inverted to Input Space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual example analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to analyze individual examples to understand what the network is using to predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dist = net.sequential[-1]\n",
    "overall_mean = th.mean(dist.class_means, dim=0)\n",
    "X_th, y, _ = next(valid_loader.__iter__())\n",
    "X_th = X_th[:,:,32:96]\n",
    "y_th = th.nn.functional.one_hot(y, num_classes=2).cuda()\n",
    "\n",
    "with th.no_grad():\n",
    "    z, lp = net(X_th.cuda())\n",
    "    lp_per_dim = dist.log_probs_per_class(z, sum_dims=False)\n",
    "    diffs = lp_per_dim[:,1] - lp_per_dim[:,0]\n",
    "    directed_diffs = (diffs * ((y * 2 - 1).unsqueeze(1).cuda()))\n",
    "    unlabeled = dist.get_unlabeled_samples(len(z))\n",
    "    threshold = 0#np.percentile(to_numpy(directed_diffs), 70)\n",
    "    mask = (directed_diffs > threshold).type_as(z)\n",
    "    #z = z * (directed_diffs > 0) + unlabeled * (directed_diffs <= 0)\n",
    "    z_class = z *  mask + overall_mean.unsqueeze(0) * (1-mask)\n",
    "    inved_class, _ = net.invert(z_class)\n",
    "    z_class, inved_lp = net(inved_class)\n",
    "    z_nonclass = z *  (1-mask) + overall_mean.unsqueeze(0) * (mask)\n",
    "    inved_nonclass, _ = net.invert(z_nonclass)\n",
    "correct = to_numpy(th.sum(directed_diffs, dim=1) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, we will do the following:\n",
    "\n",
    "1. Compute the output of the invertible network\n",
    "2. Compute for each output dimension, which class is more likely for this example\n",
    "3. Keep only the output dimensions that indicate one class, and reset the other output dimensions to neutral values: the mean between both class gaussians\n",
    "4. Invert to visualize signal parts indicative of the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(16,2))\n",
    "axes[0].plot(to_numpy(z[0].squeeze()), marker='o', color='black')\n",
    "axes[0].plot(to_numpy(dist.class_means[0].squeeze()), marker='o')\n",
    "axes[0].plot(to_numpy(dist.class_means[1].squeeze()), marker='o')\n",
    "#axes[1].plot(np.diff(to_numpy(th.softmax(lp_per_dim[0]).squeeze()), axis=0).squeeze(), marker='o',\n",
    "#        color='black')\n",
    "axes[1].plot(to_numpy(th.softmax(lp_per_dim[0], dim=0)[1]).squeeze(), marker='o',\n",
    "        color='black')\n",
    "axes[0].set_title('Example Output and Class Means')\n",
    "axes[1].set_title('Relative Probability Correct Class')\n",
    "axes[1].axhline(y=0.5, color='darkgrey')\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(16,2))\n",
    "axes[0].plot(to_numpy(z_class[0].squeeze()), marker='o', color=seaborn.color_palette()[1],)\n",
    "axes[0].plot(to_numpy(z_nonclass[0].squeeze()), marker='o', color=seaborn.color_palette()[0])\n",
    "axes[0].plot(to_numpy(overall_mean.squeeze()), marker='o', \n",
    "             color=seaborn.color_palette()[2])\n",
    "axes[1].plot(to_numpy(X_th[0].squeeze()), color='black', label=\"Real\")\n",
    "axes[1].plot(to_numpy(inved_nonclass[0].squeeze()),  color=seaborn.color_palette()[0],\n",
    "            label=\"Right Hand\")\n",
    "axes[1].plot(to_numpy(inved_class[0].squeeze()),  color=seaborn.color_palette()[1],\n",
    "            label=\"Rest\")\n",
    "axes[1].plot(to_numpy(inved_mean.squeeze()), \n",
    "             color=seaborn.color_palette()[2], label=\"Neutral\")\n",
    "axes[0].set_title('Output parts indicative of different classes')\n",
    "axes[1].set_title('Class-indicative parts inverted to input space')\n",
    "plt.legend(bbox_to_anchor=(1,-0.2,0,0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this for more examples, with correctness of prediction written as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_class in range(2):\n",
    "    fig, axes = plt.subplots(6,4, figsize=(16,8), sharex=True, sharey=True)\n",
    "    mask = y == i_class \n",
    "    i_start = 30 if i_class == 0 else 40\n",
    "    for x_orig, x_class, x_nonclass, is_correct, row_axes in zip(\n",
    "            X_th[mask][i_start:], inved_class[mask][i_start:], inved_nonclass[mask][i_start:],\n",
    "            correct[mask][i_start:], axes):\n",
    "        row_axes[0].set_ylabel(f'{[\"wrong\", \"correct\"][int(is_correct)]}', rotation=0)\n",
    "        row_axes[0].plot(to_numpy(x_orig.squeeze()), color='black')\n",
    "        row_axes[1 + i_class].plot(to_numpy(x_class.squeeze()), color=seaborn.color_palette()[i_class])\n",
    "        row_axes[1 + i_class].plot(to_numpy(inved_mean.squeeze()), color=seaborn.color_palette()[2], lw=1)\n",
    "        row_axes[2 - i_class].plot(to_numpy(x_nonclass.squeeze()), color=seaborn.color_palette()[1-i_class])\n",
    "        row_axes[2 - i_class].plot(to_numpy(inved_mean.squeeze()), color=seaborn.color_palette()[2], lw=1)\n",
    "        row_axes[3].plot(to_numpy(x_orig.squeeze()), color='black', lw=1)\n",
    "        row_axes[3].plot(to_numpy(x_class.squeeze()), color=seaborn.color_palette()[i_class], lw=1)\n",
    "        row_axes[3].plot(to_numpy(x_nonclass.squeeze()), color=seaborn.color_palette()[1-i_class], lw=1)\n",
    "        row_axes[3].plot(to_numpy(inved_mean.squeeze()), color=seaborn.color_palette()[2], lw=1)\n",
    "    axes[0][0].set_title(\"Original\")\n",
    "    axes[0][1].set_title(f\"Indicate Right\")\n",
    "    axes[0][2].set_title(f\"Indicate Rest\")\n",
    "    axes[0][3].set_title(f\"All\")\n",
    "    plt.ylim(-25,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at mispredicted inputs. For this right hand signal, it seems that the increase in amplitude towards the end causes the misprediction as resting state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_try = X_th[y==0][30:31].clone()\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(x_try.squeeze(), color='black', label='Real')\n",
    "plt.plot(inved_class[y==0][30].detach().cpu().squeeze(), label=\"Right Hand\")\n",
    "plt.plot(inved_nonclass[y==0][30].detach().cpu().squeeze(), label=\"Rest\")\n",
    "plt.plot(inved_mean.detach().cpu().squeeze(), label='Neutral')\n",
    "plt.legend(bbox_to_anchor=(1,1,0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to check that by scaling down the later part of the signal and evaluating the prediction change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "x_try = X_th[y==0][30:31].clone()\n",
    "with th.no_grad():\n",
    "    lp = net(x_try.cuda())[1]\n",
    "x_try_new = x_try.clone()\n",
    "i_time_step = 32\n",
    "block = x_try_new.data[:,:,i_time_step:].clone()\n",
    "x_try_new.data[:,:,i_time_step:] = (block - block.mean()) * 0.7 + block.mean()\n",
    "lp_orig = net(x_try.cuda())[1].squeeze()\n",
    "lp_fake  = net(x_try_new.cuda())[1].squeeze()\n",
    "plt.plot(x_try.squeeze(), color='black', label=f'Real (Correct pred: {th.softmax(lp_orig, dim=0)[0].item():.1%})')\n",
    "plt.plot(x_try_new.squeeze(), color=seaborn.color_palette()[4], \n",
    "         label=f'Manipulated (Correct pred: {th.softmax(lp_fake, dim=0)[0].item():.1%})')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For another example, the low frequencies also seem to cause the misclassification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_try = X_th[y==0][35:36].clone()\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(x_try.squeeze(), color='black', label='Real')\n",
    "plt.plot(inved_class[y==0][35].detach().cpu().squeeze(), label=\"Right Hand\")\n",
    "plt.plot(inved_nonclass[y==0][35].detach().cpu().squeeze(), label=\"Rest\")\n",
    "plt.plot(inved_mean.detach().cpu().squeeze(), label='Neutral')\n",
    "plt.legend(bbox_to_anchor=(1,1,0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check by attenuating the amplitude only in the low frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "x_try = X_th[y==0][35:36].clone()\n",
    "with th.no_grad():\n",
    "    lp = net(x_try.cuda())[1]\n",
    "x_try_new = x_try.clone()\n",
    "ffted = th.rfft(x_try_new, signal_ndim=1)\n",
    "\n",
    "ffted.data[:,:,:12] = ffted.data[:,:,:12] * 0.1\n",
    "\n",
    "x_try_new = th.irfft(ffted, signal_ndim=1, signal_sizes=[64,])\n",
    "lp_orig = net(x_try.cuda())[1].squeeze()\n",
    "lp_fake  = net(x_try_new.cuda())[1].squeeze()\n",
    "plt.plot(x_try.squeeze(), color='black', label=f'Real (Correct pred: {th.softmax(lp_orig, dim=0)[0].item():.1%})')\n",
    "plt.plot(x_try_new.squeeze(), color=seaborn.color_palette()[4], \n",
    "         label=f'Manipulated (Correct pred: {th.softmax(lp_fake, dim=0)[0].item():.1%})')\n",
    "\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
